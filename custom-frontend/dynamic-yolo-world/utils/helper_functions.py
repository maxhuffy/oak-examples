from transformers import AutoTokenizer
import os
import requests
import onnxruntime
import numpy as np
from PIL import Image

QUANT_ZERO_POINT = 90.0
QUANT_SCALE = 0.003925696481

def pad_and_quantize_features(features, max_num_classes=80):
    """
    Apply padding and quantization to feature embeddings.
    
    Args:
        features: Input feature array to be padded and quantized
        max_num_classes: Maximum number of classes for padding
    
    Returns:
        Padded and quantized features as uint8 array
    """
    num_padding = max_num_classes - features.shape[0]
    padded_features = np.pad(
        features, ((0, num_padding), (0, 0)), mode="constant"
    ).T.reshape(1, 512, max_num_classes)
    quantized_features = (padded_features / QUANT_SCALE) + QUANT_ZERO_POINT
    quantized_features = quantized_features.astype("uint8")
    return quantized_features


def extract_text_embeddings(class_names, max_num_classes=80):
    tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    text = tokenizer(text=class_names, return_tensors="np", padding=True)
    text_onnx = text["input_ids"].astype(np.int64)
    attention_mask = (text_onnx != 0).astype(np.int64)

    textual_onnx_model_path = download_model(
        "https://huggingface.co/jmzzomg/clip-vit-base-patch32-text-onnx/resolve/main/model.onnx",
        "clip_textual_hf.onnx",
    )

    session_textual = onnxruntime.InferenceSession(
        textual_onnx_model_path,
        providers=[
            "TensorrtExecutionProvider",
            "CUDAExecutionProvider",
            "CPUExecutionProvider",
        ],
    )
    textual_output = session_textual.run(
        None,
        {
            session_textual.get_inputs()[0].name: text_onnx,
            "attention_mask": attention_mask,
        },
    )[0]

    text_features = pad_and_quantize_features(textual_output, max_num_classes)

    del session_textual

    return text_features

def extract_image_prompt_embeddings(image, max_num_classes=80):
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)

    input_tensor = preprocess_image(image)

    onnx_model_path = download_model(
        "https://huggingface.co/sokovninn/clip-visual-with-projector/resolve/main/clip_visual_with_projector.onnx",
        "clip_visual_with_projector.onnx",
    )
    
    session = onnxruntime.InferenceSession(
        onnx_model_path,
        providers=[
            "TensorrtExecutionProvider",
            "CUDAExecutionProvider", 
            "CPUExecutionProvider"
        ]
    )
    
    input_name = session.get_inputs()[0].name
    outputs = session.run(None, {input_name: input_tensor})
    image_embeddings = outputs[0]  # Shape: (1, 512)
    
    image_embeddings = image_embeddings.squeeze(0).reshape(1, -1)  # Shape: (1, 512)
    
    image_features = pad_and_quantize_features(image_embeddings, max_num_classes)
    
    del session
    
    return image_features



def download_model(url, save_path):
    if not os.path.exists(save_path):
        print(f"Downloading model from {url}...")
        response = requests.get(url)
        if response.status_code == 200:
            with open(save_path, "wb") as f:
                f.write(response.content)
            print(f"Model saved to {save_path}.")
        else:
            raise Exception(
                f"Failed to download model. Status code: {response.status_code}"
            )
    else:
        print(f"Model already exists at {save_path}.")

    return save_path

def preprocess_image(image):
    """Preprocess image for CLIP vision model input"""
    image = image.resize((224, 224))
    
    # Convert to numpy array and normalize
    image_array = np.array(image).astype(np.float32) / 255.0
    
    # CLIP normalization values
    mean = np.array([0.48145466, 0.4578275, 0.40821073])
    std = np.array([0.26862954, 0.26130258, 0.27577711])
    
    # Normalize
    image_array = (image_array - mean) / std
    
    # Convert to CHW format and add batch dimension
    image_array = np.transpose(image_array, (2, 0, 1))  # HWC to CHW
    image_array = np.expand_dims(image_array, axis=0)   # Add batch dimension
    
    return image_array.astype(np.float32)
